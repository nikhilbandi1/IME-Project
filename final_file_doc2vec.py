# -*- coding: utf-8 -*-
"""Final file doc2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YY_xQea2s3r59ACHNx3yyMrj17rUYyhI

# **Installation and Importing**
"""

pip install python-docx

pip install nltk

pip install gensim==3.6.0

pip install python-Levenshtein

import docx
from docx import Document
import pandas as pd
import string
import numpy as np
import pickle
import nltk
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import gensim
from gensim.models import doc2vec

import sklearn
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import random
import seaborn as sns

nltk.download('punkt')
nltk.download('stopwords')

from google.colab import drive
drive.mount('/content/drive')

"""# Data Importing
---


"""

data = pd.read_excel('/content/drive/MyDrive/Med_Dep_Id/Models/Copy of excel cleaned data.xlsx')

data = pd.DataFrame(data)

# (data)

data = data.drop("Unnamed: 1", axis =1)

# data

# converting in lower case
data["Departments"]= [doc.lower() for doc in data["Departments"]]
data["Description"]= [doc.lower() for doc in data["Description"]]
data["Description"] = [x.replace('_', ' ') for x in data["Description"]]
data["Description"] = [x.replace('-', ' ') for x in data["Description"]]

data["Departments"] =[x.lstrip() for x in data["Departments"]]
# removing puntuations
def remove_punct(txt):
    new_txt="".join([c for c in txt if c not in string.punctuation])
    return new_txt

data['message_clean']=data['Description'].apply(lambda x:remove_punct(x))

# data

# converting sentences in words

words_token=[word_tokenize(doc) for doc in data['message_clean']]
data['tokens']=words_token
# pd.set_option('display.max_colwidth',None)
# pd.set_option('display.max_rows',None)
# data

stopwords=nltk.corpus.stopwords.words('english')

def remove_stopwords(txt):
    txt_clean=[word for word in txt if word not in stopwords]
    return txt_clean

data['tokens_refined']=data['tokens'].apply(lambda x:remove_stopwords(x))

data

# result = result.reset_index()
# for splitting
import sklearn
x_train,x_test,y_train,y_test=sklearn.model_selection.train_test_split(data['tokens_refined'],data['Departments'],test_size=0.3)  #without stratification very high accuracy is being achieved for 5 departments
# ,stratify = combined_df['Departments'] add once data is cleaned

training_set=pd.DataFrame({'Departments':y_train,'tokens_refined':x_train})
testing_set=pd.DataFrame({'Departments':y_test,'tokens_refined':x_test})

import pandas as pd
deptt= pd.read_csv("/content/drive/MyDrive/Med_Dep_Id/deptt_cleaned.csv")
deptt.rename(columns = {'message_refined':'tokens_refined'}, inplace = True)

deptt = pd.DataFrame({'Departments':deptt["Departments"],'tokens_refined':deptt["tokens_refined"]})
# print(deptt)

#  adding department data to the training dataset
pd.concat([training_set, deptt], axis=0,ignore_index = True)

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import gensim
from gensim.models import doc2vec

def tagged_document(list_of_ListOfWords):
    for x in list_of_ListOfWords.index:
        yield doc2vec.TaggedDocument(training_set["tokens_refined"][x], [training_set["Departments"][x]])

data_train = list(tagged_document(x_train))

"""# model initialization
---


"""

model = Doc2Vec(data_train, 
                vector_size=40, # Dimensionality of the generated feature vectors
                window=10, # The maximum distance between the current and predicted word within a sentence
                min_count=1, #Ignores all words with total frequency lower than this
                workers=5,  # Number of worker threads to train the model
                epochs = 1000,
                alpha=0.025,  # The initial learning rate
                min_alpha=0.00025,  # Learning rate will linearly drop to min_alpha as training progresses
                dm=1)  # dm=1 means 'distributed memory' (PV-DM:predict a center word from the randomly
                                                                        # sampled set of words by taking as input — 
                                                                        # the context words and a paragraph id.))
filename = '/content/drive/MyDrive/Med_Dep_Id/Models/final_doc2vec_excel_data.model'

# Save trained doc2vec model
filename = '/content/drive/MyDrive/Med_Dep_Id/Models/final_doc2vec_excel_data.model'
model.save(filename)

# Load saved doc2vec model
filename = '/content/drive/MyDrive/Med_Dep_Id/Models/final_doc2vec_excel_data.model'
model= Doc2Vec.load(filename)

print(model)

len(model.wv.vocab)

model.infer_vector('myopathy')

model.infer_vector('myopathy')

model.infer_vector(x_test[0])

# test_doc = word_tokenize("heart blood".lower())
# model.docvecs.most_similar(positive=[model.infer_vector(test_doc)],topn=10)

count=0
for i in x_test.index:
    z=model.docvecs.most_similar(positive=[model.infer_vector(x_test[i])],topn=3) #looking accuracy if the top result matches based 
    # print(z)
    for j in z:
        # print(j)
        if j[0]==y_test[i]:
            # print(j[0]," ",y_test[i])
            count+=1

print(count, len(y_test))

model.infer_vector(x_test[0])

# x_train
# test_doc = word_tokenize("heart blood".lower())
# model.docvecs.most_similar(positive=[model.infer_vector(test_doc)],topn=10)

count=0
for i in x_train.index:
    z=model.docvecs.most_similar(positive=[model.infer_vector(x_train[i])],topn=3) #looking accuracy if the top result matches based 
    # print(z)
    for j in z:
        # print(j)
        if j[0]==y_train[i]:
            # print(j[0]," ",y_test[i])
            count+=1

print(count, len(y_train))

print(len(model.wv.vocab))
print(len(model.docvecs))